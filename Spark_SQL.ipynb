{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL\n",
    "\n",
    "<div style=\"text-align: right\">\n",
    "2018.11.03 / 발제자 임지훈\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "### Reference\n",
    "- 빅데이터 분석을 위한 스파크2 프로그래밍, 백성민, 위키북스\n",
    "- Spark SQL, DataFrames and Datasets Guide, https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "- Pyspark Package, http://spark.apache.org/docs/latest/api/python/pyspark.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "### 순서\n",
    "1. Dataset\n",
    "2. Word Count in DataFrame\n",
    "3. DataFrame 생성\n",
    "4. 주요 연산\n",
    "5. Pandas 연동\n",
    "6. Hive 연동\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD 생성 후 map() 메소드를 이용해보자\n",
    "rdd = sc.parallelize(range(20))\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.map(lambda element : element+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Word Count in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"sample\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|A bad penny alway...|\n",
      "|A barking dog nev...|\n",
      "|A bird in the han...|\n",
      "|A cat may look at...|\n",
      "|A chain is only a...|\n",
      "|A change is as go...|\n",
      "|A dog is a man's ...|\n",
      "|A drowning man wi...|\n",
      "|A fish always rot...|\n",
      "|A fool and his mo...|\n",
      "|A friend in need ...|\n",
      "|A golden key can ...|\n",
      "|A good beginning ...|\n",
      "|A good man is har...|\n",
      "|A house divided a...|\n",
      "|A person is known...|\n",
      "|A house is not a ...|\n",
      "|A journey of a th...|\n",
      "|A leopard cannot ...|\n",
      "|A little knowledg...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "source = \"file:///home/ubuntu/18-2Engineering/Week05_181103/resources/countMe.txt\"         \n",
    "df = spark.read.text(source)  \n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordDF= df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   word|\n",
      "+-------+\n",
      "|      A|\n",
      "|    bad|\n",
      "|  penny|\n",
      "| always|\n",
      "|  turns|\n",
      "|     up|\n",
      "|      A|\n",
      "|barking|\n",
      "|    dog|\n",
      "|  never|\n",
      "|  bites|\n",
      "|      A|\n",
      "|   bird|\n",
      "|     in|\n",
      "|    the|\n",
      "|   hand|\n",
      "|     is|\n",
      "|  worth|\n",
      "|    two|\n",
      "|     in|\n",
      "+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=wordDF.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      July|    1|\n",
      "|     those|    5|\n",
      "|     spoil|    3|\n",
      "|    travel|    2|\n",
      "|       few|    1|\n",
      "|pack-drill|    1|\n",
      "|    waters|    1|\n",
      "|    harder|    1|\n",
      "|      hope|    1|\n",
      "|      some|    1|\n",
      "|    taking|    1|\n",
      "|   Sabbath|    1|\n",
      "|     parts|    1|\n",
      "|      lies|    1|\n",
      "|    Mighty|    1|\n",
      "|  Tomorrow|    2|\n",
      "|   vinegar|    1|\n",
      "|   stomach|    2|\n",
      "|   showers|    2|\n",
      "|   flowers|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = \"file:///home/ubuntu/18-2Engineering/Week05_181103/output/\"\n",
    "result.write.format(\"csv\").save(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|      word|count|\n",
      "+----------+-----+\n",
      "|      July|    1|\n",
      "|     those|    5|\n",
      "|     spoil|    3|\n",
      "|    travel|    2|\n",
      "|       few|    1|\n",
      "|pack-drill|    1|\n",
      "|    waters|    1|\n",
      "|    harder|    1|\n",
      "|      hope|    1|\n",
      "|      some|    1|\n",
      "|    taking|    1|\n",
      "|   Sabbath|    1|\n",
      "|     parts|    1|\n",
      "|      lies|    1|\n",
      "|    Mighty|    1|\n",
      "|  Tomorrow|    2|\n",
      "|   vinegar|    1|\n",
      "|   stomach|    2|\n",
      "|   showers|    2|\n",
      "|   flowers|    2|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/home/ubuntu/18-2Engineering/Week05_181103/output already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.save.\n: org.apache.spark.sql.AnalysisException: path file:/home/ubuntu/18-2Engineering/Week05_181103/output already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-13294c451826>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moutdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"file:///home/ubuntu/18-2Engineering/Week05_181103/output/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/home/ubuntu/18-2Engineering/Week05_181103/output already exists.;'"
     ]
    }
   ],
   "source": [
    "#DataFrame을 생성 후 SQL의 SELECT 문으로 원하는 데이터를 조회해보자\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"sample\")\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "source = \"file:///home/ubuntu/18-2Engineering/Week05_181103/resources/countMe.txt\"         \n",
    "df = spark.read.text(source)  \n",
    "\n",
    "wordDF = df.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "result = wordDF.groupBy(\"word\").count()\n",
    "result.show()\n",
    "\n",
    "outdir = \"file:///home/ubuntu/18-2Engineering/Week05_181103/output/\"\n",
    "result.write.format(\"csv\").save(outdir)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 아웃풋 디렉토리로 가서 결과가 잘 저장됐는지 확인해보세요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import functions\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "#from word import Word\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sample\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "#    .config(\"spark.sql.warehouse.dir\", \"file:///Users/beginspark/Temp/\") \\\n",
    "#    .config(\"spark.driver.host\", \"127.0.0.1\") \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1.외부 데이터 소스로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "+------+--------------+----------------+\n",
      "|  name|favorite_color|favorite_numbers|\n",
      "+------+--------------+----------------+\n",
      "|Alyssa|          null|  [3, 9, 15, 20]|\n",
      "|   Ben|           red|              []|\n",
      "+------+--------------+----------------+\n",
      "\n",
      "+-----------+\n",
      "|      value|\n",
      "+-----------+\n",
      "|Michael, 29|\n",
      "|   Andy, 30|\n",
      "| Justin, 19|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "commonDir = \"file:///home/ubuntu/18-2Engineering/Week05_181103/\"\n",
    "df1 = spark.read.json(commonDir + \"./resources/people.json\")\n",
    "df2 = spark.read.parquet(commonDir + \"./resources/users.parquet\")\n",
    "df3 = spark.read.text(commonDir + \"./resources/people.txt\")\n",
    "\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. RDD로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "|age|    job|    name|\n",
      "+---+-------+--------+\n",
      "| 26|googler|soonchan|\n",
      "| 25| mother|   taeoh|\n",
      "| 25| father| hyunwoo|\n",
      "| 25|garbage| euntaek|\n",
      "|  2|student|  jihoon|\n",
      "+---+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row1 = Row(name=\"soonchan\", age=26, job=\"googler\")\n",
    "row2 = Row(name=\"taeoh\", age=25, job=\"mother\")\n",
    "row3 = Row(name=\"hyunwoo\", age=25, job=\"father\")\n",
    "row4 = Row(name=\"euntaek\", age=25, job=\"garbage\")\n",
    "row5 = Row(name=\"jihoon\", age=2, job=\"student\")\n",
    "\n",
    "data = [row1, row2, row3, row4, row5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "df4 = spark.createDataFrame(data)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3 List로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------+\n",
      "|age|    job|    name|\n",
      "+---+-------+--------+\n",
      "| 26|googler|soonchan|\n",
      "| 25| mother|   taeoh|\n",
      "| 25| father| hyunwoo|\n",
      "| 25|garbage| euntaek|\n",
      "|  2|student|  jihoon|\n",
      "+---+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "row1 = Row(name=\"soonchan\", age=26, job=\"googler\")\n",
    "row2 = Row(name=\"taeoh\", age=25, job=\"mother\")\n",
    "row3 = Row(name=\"hyunwoo\", age=25, job=\"father\")\n",
    "row4 = Row(name=\"euntaek\", age=25, job=\"garbage\")\n",
    "row5 = Row(name=\"jihoon\", age=2, job=\"student\")\n",
    "\n",
    "data = [row1, row2, row3, row4,row5]\n",
    "\n",
    "df5 = spark.createDataFrame(data)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-4 Schema 지정을 통한 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    name|age|    job|\n",
      "+--------+---+-------+\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sf1 = StructField(\"name\", StringType(), True)\n",
    "sf2 = StructField(\"age\", IntegerType(), True)\n",
    "sf3 = StructField(\"job\", StringType(), True)\n",
    "\n",
    "schema = StructType([sf1, sf2, sf3])\n",
    "\n",
    "r1 = (\"soonchan\", 26, \"googler\")\n",
    "r2 = (\"taeoh\", 25, \"mother\")\n",
    "r3 = (\"hyunwoo\", 25, \"father\")\n",
    "r4 = (\"euntaek\", 25, \"garbage\")\n",
    "r5 = (\"jihoon\", 25, \"student\")\n",
    "rows = [r1, r2, r3, r4, r5]\n",
    "\n",
    "df6 = spark.createDataFrame(rows, schema)\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-5. 튜플로부터 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    name|age|    job|\n",
      "+--------+---+-------+\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#네임드튜플(namedtuple)로 먼저 row와 column에 해당되는 부분을 정의한다.\n",
    "Person = collections.namedtuple('Person', 'name age job')\n",
    "\n",
    "# sample dataframe 1\n",
    "row1 = Person(name=\"soonchan\", age=26, job=\"googler\")\n",
    "row2 = Person(name=\"taeoh\", age=25, job=\"mother\")\n",
    "row3 = Person(name=\"hyunwoo\", age=25, job=\"father\")\n",
    "row4 = Person(name=\"euntaek\", age=25, job=\"garbage\")\n",
    "row5 = Person(name=\"jihoon\", age=25, job=\"student\")\n",
    "\n",
    "data = [row1, row2, row3, row4, row5]\n",
    "\n",
    "df7 = spark.createDataFrame(data)\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|   note|    20| 2000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|   note|    15| 1000|\n",
      "|store1|    pen|    20| 5000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d1 = (\"store2\", \"note\", 20, 2000)\n",
    "d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "d3 = (\"store1\", \"note\", 15, 1000)\n",
    "d4 = (\"store1\", \"pen\", 20, 5000)\n",
    "df8 = spark.createDataFrame([d1, d2, d3, d4]).toDF(\"store\", \"product\", \"amount\", \"price\")\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-7. class를 이용하여 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|count|word|\n",
      "+-----+----+\n",
      "|    1|  w1|\n",
      "|    1|  w2|\n",
      "+-----+----+\n",
      "\n",
      "+-----+----+\n",
      "|count|word|\n",
      "+-----+----+\n",
      "|    1|  w1|\n",
      "|    1|  w3|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#사용자가 class를 정의한 후, 새로운 객체를 생성하고 그 객체를 이용해서 Dataframe으로 생성\n",
    "class Word:\n",
    "    def __init__(self, word, count):\n",
    "        self.word = word\n",
    "        self.count = count\n",
    "\n",
    "ldf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w2\", 1)])\n",
    "rdf = spark.createDataFrame([Word(\"w1\", 1), Word(\"w3\", 1)])\n",
    "ldf.show()\n",
    "rdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# createDataFrame\n",
    "def createDataFrame(spark, sc):\n",
    "    sparkHomeDir = \"file:///home/ubuntu/18-2Engineering/Week05_181103\"\n",
    "\n",
    "    # 1. 파일로 부터 생성\n",
    "    df1 = spark.read.json(sparkHomeDir + \"./resources/people.json\")\n",
    "    df2 = spark.read.parquet(sparkHomeDir + \"./resources/users.parquet\")\n",
    "    df3 = spark.read.text(sparkHomeDir + \"./resources/people.txt\")\n",
    "\n",
    "    # 2. List으로부터 생성 (ex5-17)\n",
    "    row1 = Row(name=\"soonchan\", age=26, job=\"googler\")\n",
    "    row2 = Row(name=\"taeoh\", age=25, job=\"mother\")\n",
    "    row3 = Row(name=\"hyunwoo\", age=25, job=\"father\")\n",
    "    row4 = Row(name=\"euntaek\", age=25, job=\"garbage\")\n",
    "    row5 = Row(name=\"jihoon\", age=25, job=\"student\")\n",
    "    data = [row1, row2, row3, row4,row5]\n",
    "    df4 = spark.createDataFrame(data)\n",
    "\n",
    "    # 3. RDD로부터 생성 (ex5-20)\n",
    "    rdd = spark.sparkContext.parallelize(data)\n",
    "    df5 = spark.createDataFrame(data)\n",
    "\n",
    "    # 4. 스키마 지정(ex5-23)\n",
    "    sf1 = StructField(\"name\", StringType(), True)\n",
    "    sf2 = StructField(\"age\", IntegerType(), True)\n",
    "    sf3 = StructField(\"job\", StringType(), True)\n",
    "    schema = StructType([sf1, sf2, sf3])\n",
    "    r1 = (\"soonchan\", 26, \"googler\")\n",
    "    r2 = (\"taeoh\", 25, \"mother\")\n",
    "    r3 = (\"hyunwoo\", 25, \"father\")\n",
    "    r4 = (\"euntaek\", 25, \"garbage\")\n",
    "    r5 = (\"jihoon\", 25, \"student\")\n",
    "    rows = [r1, r2, r3, r4, r5]\n",
    "    df6 = spark.createDataFrame(rows, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 주요 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1. 액션 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    name|age|    job|\n",
      "+--------+---+-------+\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show()\n",
    "# DataFrame에 저장된 데이터를 화면에 출력\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "head() / first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='soonchan', age=26, job='googler')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# head, first\n",
    "# 첫 번째 로우를 리턴\n",
    "df7.head()\n",
    "# df8.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(store='store2', product='note', amount=20, price=2000),\n",
       " Row(store='store2', product='bag', amount=10, price=5000),\n",
       " Row(store='store1', product='note', amount=15, price=1000)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take(n)\n",
    "# 첫 n개를 보여줌\n",
    "df8.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "# RDD의 count()연산과 동일, 로우의 개수를 리턴\n",
    "df5.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='soonchan', age=26, job='googler'),\n",
       " Row(name='taeoh', age=25, job='mother'),\n",
       " Row(name='hyunwoo', age=25, job='father'),\n",
       " Row(name='euntaek', age=25, job='garbage'),\n",
       " Row(name='jihoon', age=25, job='student')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect\n",
    "# 데이터셋에 있는 데이터를 로컬 컬렉션(배열) 형태로 리턴\n",
    "# 메모리 부족 에러가 발생하지 않도록 주의!\n",
    "df7.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|                age|\n",
      "+-------+-------------------+\n",
      "|  count|                  5|\n",
      "|   mean|               25.2|\n",
      "| stddev|0.44721359549995765|\n",
      "|    min|                 25|\n",
      "|    max|                 26|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe\n",
    "df7.describe(\"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|summary|    job|\n",
      "+-------+-------+\n",
      "|  count|      5|\n",
      "|   mean|   null|\n",
      "| stddev|   null|\n",
      "|    min| father|\n",
      "|    max|student|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.describe(\"job\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2. 기본 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "persist() / cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[name: string, age: bigint, job: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cache는 persist와 동일한 기능 제공\n",
    "# 작업중인 데이터를 메모리에 저장\n",
    "\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "df7.persist(StorageLevel.MEMORY_AND_DISK_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://스파크셸실행시킨서버:4040 으로 접속"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://스파크셸실행시킨서버:4040 다시 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printSchema(), columns, dtypes, schema\n",
    "# 스키마 정보를 조회하는 메서드와 리스트들\n",
    "\n",
    "df7.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'job']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메서드가 아닙니다. 리스트를 반환합니다.\n",
    "df7.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint'), ('job', 'string')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메서드가 아닙니다. 리스트를 반환합니다.\n",
    "df7.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(name,StringType,true),StructField(age,LongType,true),StructField(job,StringType,true)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 메서드가 아닙니다. 리스트를 반환합니다.\n",
    "df7.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createOrReplaceTempView()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|soonchan| 26|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# createOrReplaceTempView()\n",
    "\n",
    "# DataFrame을 SQL문을 이용해 테이블처럼 조회할 수 있도록 등록해줍니다.\n",
    "# 단, 이 메서드로 등록된 테이블은 스파크세션이 유지되는 동안만 유효합니다.\n",
    "# (세션이 종료되면 사라짐)\n",
    "\n",
    "df7.createOrReplaceTempView(\"engineeringTeam\")\n",
    "spark.sql(\"SELECT name, age FROM engineeringTeam WHERE age > 25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Filter (isnotnull(age#133L) && (age#133L > 25))\n",
      "+- InMemoryTableScan [name#132, age#133L], [isnotnull(age#133L), (age#133L > 25)]\n",
      "      +- InMemoryRelation [name#132, age#133L, job#134], true, 10000, StorageLevel(disk, memory, 2 replicas)\n",
      "            +- Scan ExistingRDD[name#132,age#133L,job#134]\n"
     ]
    }
   ],
   "source": [
    "# explain()\n",
    "\n",
    "# 데이터프레임 처리와 관련된 실행 계획 정보를 출력합니다.\n",
    "\n",
    "spark.sql(\"SELECT name, age FROM engineeringTeam WHERE age > 25\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3. Row, Column, functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    name|age|    job|\n",
      "+--------+---+-------+\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SQL문을 직접 이용한 연산\n",
    "\n",
    "spark.sql(\"SELECT name, age, job FROM engineeringTeam WHERE age > 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    name|age|    job|\n",
      "+--------+---+-------+\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이 연산과 동일합니다.\n",
    "# 이런 형태의 연산을 비타입 트랜스포메이션 연산이라 합니다.\n",
    "\n",
    "df7.where(df7['age'] > 20).show()\n",
    "#df7.where(df7.age > 20).show() # 이런 형태도 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias(), as()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(age + 1)|\n",
      "+---------+\n",
      "|       27|\n",
      "|       26|\n",
      "|       26|\n",
      "|       26|\n",
      "|       26|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alias()\n",
    "\n",
    "# 컬럼 명에 원하는 이름을 붙여줄 수 있습니다. \n",
    "df7.select(df7['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 27|\n",
      "| 26|\n",
      "| 26|\n",
      "| 26|\n",
      "| 26|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.select((df7['age'] + 1).alias(\"age\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "isin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  3|\n",
      "|  5|\n",
      "|  7|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# isIn()\n",
    "\n",
    "# 컬럼의 값이 인자로 지정된 값에 포함되어 있는지의 여부를 확인합니다.\n",
    "# 아래의 예제는 유효한 값을 broadcaast 변수에 담아 필터처럼 사용합니다.\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "nums = spark.sparkContext.broadcast([1, 3, 5, 7, 9])\n",
    "rdd = spark.sparkContext.parallelize(range(0, 10)).map(lambda v: Row(v))\n",
    "df = spark.createDataFrame(rdd)\n",
    "df.where(df._1.isin(nums.value)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|    name|elder|\n",
      "+--------+-----+\n",
      "|soonchan| true|\n",
      "|   taeoh|false|\n",
      "| hyunwoo|false|\n",
      "| euntaek|false|\n",
      "|  jihoon|false|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elder = spark.sparkContext.broadcast([26,27,28,29,30])\n",
    "df7.select(df7['name'], df7['age'].isin(elder.value).alias(\"elder\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "when()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+\n",
      "| id|type|\n",
      "+---+----+\n",
      "|  0|even|\n",
      "|  1| odd|\n",
      "|  2|even|\n",
      "|  3| odd|\n",
      "|  4|even|\n",
      "+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when()\n",
    "\n",
    "from pyspark.sql import functions\n",
    "\n",
    "ds = spark.range(0, 5)\n",
    "col = functions.when(ds.id % 2 == 0, \"even\").otherwise(\"odd\").alias(\"type\")\n",
    "ds.select(ds.id, col).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(store='store1', min(amount)=15), Row(store='store2', min(amount)=10)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = (\"store2\", \"note\", 20, 2000)\n",
    "d2 = (\"store2\", \"bag\", 10, 5000)\n",
    "d3 = (\"store1\", \"note\", 15, 1000)\n",
    "d4 = (\"store1\", \"pen\", 20, 5000)\n",
    "df = spark.createDataFrame([d1, d2, d3, d4]).toDF(\"store\", \"product\", \"amount\", \"price\")\n",
    "\n",
    "gdf = df.groupBy(df.store)\n",
    "sorted(gdf.agg({\"*\": \"count\"}).collect())\n",
    "\n",
    "sorted(gdf.agg(functions.min(df.amount)).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max(), mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|      26|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.select(max(df7['age'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(job)|\n",
      "+--------+\n",
      "| student|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.select(max(df7['job'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(age)|\n",
      "+--------+\n",
      "|    25.2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.select(mean(df7['age'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|avg(job)|\n",
      "+--------+\n",
      "|    null|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df7.select(mean(df7['job'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect_list() , collect_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+\n",
      "|    name|age|    job|\n",
      "+--------+---+-------+\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "|soonchan| 26|googler|\n",
      "|   taeoh| 25| mother|\n",
      "| hyunwoo| 25| father|\n",
      "| euntaek| 25|garbage|\n",
      "|  jihoon| 25|student|\n",
      "+--------+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 특정 컬럼 값을 모아서 하나의 리스트 또는 세트(set)로 된 컬럼을 생성함\n",
    "# 리스트는 중복 포함, 세트는 중복 미포함\n",
    "\n",
    "doubleDf = df7.union(df7)\n",
    "doubleDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_list(name)=['soonchan', 'taeoh', 'hyunwoo', 'euntaek', 'jihoon', 'soonchan', 'taeoh', 'hyunwoo', 'euntaek', 'jihoon'])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleDf.select(collect_list('name')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(collect_set(name)=['hyunwoo', 'euntaek', 'taeoh', 'soonchan', 'jihoon'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doubleDf.select(collect_set('name')).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|collect_list(name)[0]|\n",
      "+---------------------+\n",
      "|             soonchan|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doubleDf.select(collect_list(\"name\")[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|collect_set(name)[4]|\n",
      "+--------------------+\n",
      "|              jihoon|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doubleDf.select(collect_set(\"name\")[4]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() / countDistinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|count(name)|count(DISTINCT name)|\n",
      "+-----------+--------------------+\n",
      "|         10|                   5|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doubleDf.select(count(\"name\"), countDistinct(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|   note|    20| 2000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|   note|    15| 1000|\n",
      "|store1|    pen|    20| 5000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|sum(price)|\n",
      "+----------+\n",
      "|     13000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(sum(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|(sum(price) / count(price))|\n",
      "+---------------------------+\n",
      "|                     3250.0|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(sum(\"price\") / count(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "round() / sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|SQRT((sum(price) / count(price)))|\n",
      "+---------------------------------+\n",
      "|                 57.0087712549569|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(sqrt(sum(\"price\") / count(\"price\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|round(SQRT((sum(price) / count(price))), 3)|\n",
      "+-------------------------------------------+\n",
      "|                                     57.009|\n",
      "+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(round(sqrt(sum(\"price\") / count(\"price\")),3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "desc() / asc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|    pen|    20| 5000|\n",
      "|store2|   note|    20| 2000|\n",
      "|store1|   note|    15| 1000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.sort(desc(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store1|    pen|    20| 5000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store2|   note|    20| 2000|\n",
      "|store1|   note|    15| 1000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.sort(desc(\"price\"),asc(\"store\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|max(amount)|min(price)|\n",
      "+-----------+----------+\n",
      "|         20|      1000|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.agg(max(\"amount\"), min(\"price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|max(amount)|min(price)|\n",
      "+-----------+----------+\n",
      "|         20|      1000|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select(max(\"amount\"), min(\"price\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "groupBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "| store|sum(price)|\n",
      "+------+----------+\n",
      "|store2|      7000|\n",
      "|store1|      6000|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.groupBy(\"store\").sum(\"price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "| store|product|sum(price)|\n",
      "+------+-------+----------+\n",
      "|store1|   note|      1000|\n",
      "|store2|    bag|      5000|\n",
      "|store1|    pen|      5000|\n",
      "|store2|   note|      2000|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.groupBy(\"store\", \"product\").sum(\"price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+\n",
      "| store|product|sum(price)|\n",
      "+------+-------+----------+\n",
      "|  null|   null|     13000|\n",
      "|  null|    bag|      5000|\n",
      "|  null|   note|      3000|\n",
      "|  null|    pen|      5000|\n",
      "|store1|   null|      6000|\n",
      "|store1|   note|      1000|\n",
      "|store1|    pen|      5000|\n",
      "|store2|   null|      7000|\n",
      "|store2|    bag|      5000|\n",
      "|store2|   note|      2000|\n",
      "+------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.cube(\"store\", \"product\").sum(\"price\")\\\n",
    ".sort(asc(\"store\"), asc(\"product\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-----+\n",
      "| store|product|amount|price|\n",
      "+------+-------+------+-----+\n",
      "|store2|   note|    20| 2000|\n",
      "|store2|    bag|    10| 5000|\n",
      "|store1|   note|    15| 1000|\n",
      "|store1|    pen|    20| 5000|\n",
      "+------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.select([c for c in df8.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Pandas 연동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /home/ubuntu/anaconda3/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.14 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pyarrow)\n",
      "Requirement already satisfied: six>=1.0.0 in /home/ubuntu/anaconda3/lib/python3.6/site-packages (from pyarrow)\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'amount': [20, 10, 15, 20],\n",
       " 'name': ['store2', 'store2', 'store1', 'store1'],\n",
       " 'price': [2000, 5000, 1000, 5000],\n",
       " 'product': ['note', 'bag', 'note', 'pen']}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "sales_data = {'name' : ['store2', 'store2', 'store1', 'store1'],\n",
    "             'product' : ['note', 'bag', 'note', 'pen'],\n",
    "             'amount' : [20, 10, 15, 20],\n",
    "             'price' : [2000, 5000, 1000, 5000]}\n",
    "sales_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>store2</td>\n",
       "      <td>2000</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>store2</td>\n",
       "      <td>5000</td>\n",
       "      <td>bag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15</td>\n",
       "      <td>store1</td>\n",
       "      <td>1000</td>\n",
       "      <td>note</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>store1</td>\n",
       "      <td>5000</td>\n",
       "      <td>pen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount    name  price product\n",
       "0      20  store2   2000    note\n",
       "1      10  store2   5000     bag\n",
       "2      15  store1   1000    note\n",
       "3      20  store1   5000     pen"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.DataFrame(sales_data)\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas Dataframe -> pyspark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-------+\n",
      "|amount|  name|price|product|\n",
      "+------+------+-----+-------+\n",
      "|    20|store2| 2000|   note|\n",
      "|    10|store2| 5000|    bag|\n",
      "|    15|store1| 1000|   note|\n",
      "|    20|store1| 5000|    pen|\n",
      "+------+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(pdf)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  name|count|\n",
      "+------+-----+\n",
      "|store2|    2|\n",
      "|store1|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df =spark.createDataFrame(pdf).groupBy(\"name\").count()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " pyspark DataFrame -> Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>store2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>store1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  count\n",
       "0  store2      2\n",
       "1  store1      2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf2 = df.toPandas()\n",
    "pdf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-------+\n",
      "|amount|  name|price|product|\n",
      "+------+------+-----+-------+\n",
      "|    20|store2| 2000|   note|\n",
      "|    10|store2| 5000|    bag|\n",
      "|    15|store1| 1000|   note|\n",
      "|    20|store1| 5000|    pen|\n",
      "+------+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totalDf = spark.createDataFrame(pdf)\n",
    "totalDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-----+-------+-----------+\n",
      "|amount|  name|price|product|total_price|\n",
      "+------+------+-----+-------+-----------+\n",
      "|    20|store2| 2000|   note|      40000|\n",
      "|    10|store2| 5000|    bag|      50000|\n",
      "|    15|store1| 1000|   note|      15000|\n",
      "|    20|store1| 5000|    pen|     100000|\n",
      "+------+------+-----+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_total_price(amount, price):\n",
    "    return amount * price\n",
    "\n",
    "total_price = pandas_udf(get_total_price, returnType=LongType())\n",
    "totalDf.withColumn(\"total_price\", total_price(totalDf[\"amount\"], totalDf[\"price\"])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hive 연동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://creativedata.atlassian.net/wiki/spaces/SAP/pages/82255289/Pyspark+-+Read+Write+files+from+Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "\n",
    "SparkContext.setSystemProperty(\"hive.metastore.uris\", \"thrift://nn1:9083\")\n",
    "\n",
    "sparkSession = (SparkSession\n",
    "                .builder\n",
    "                .appName('example-pyspark-read-and-write-from-hive')\n",
    "                .enableHiveSupport()\n",
    "                .getOrCreate())\n",
    "\n",
    "row1 = Person(name=\"soonchan\", age=26, job=\"googler\")\n",
    "row2 = Person(name=\"taeoh\", age=25, job=\"mother\")\n",
    "row3 = Person(name=\"hyunwoo\", age=25, job=\"father\")\n",
    "row4 = Person(name=\"euntaek\", age=25, job=\"garbage\")\n",
    "row5 = Person(name=\"jihoon\", age=25, job=\"student\")\n",
    "\n",
    "data = [row1, row2, row3, row4, row5]\n",
    "\n",
    "df = sparkSession.createDataFrame(data)\n",
    "\n",
    "df.write.saveAsTable('example')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do Yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.5.2.1.1절 ~ 5.5.2.2.4절\n",
    "def runBasicOpsEx(spark, sc, df):\n",
    "    df.show()\n",
    "    df.head()\n",
    "    df.first()\n",
    "    df.take(2)\n",
    "    df.count()\n",
    "    df.collect()\n",
    "    df.describe(\"age\").show()\n",
    "    df.persist(StorageLevel.MEMORY_AND_DISK_2)\n",
    "    df.printSchema()\n",
    "    df.columns\n",
    "    df.dtypes\n",
    "    df.schema\n",
    "    df.createOrReplaceTempView(\"users\")\n",
    "    spark.sql(\"select name, age from users where age > 20\").show()\n",
    "    spark.sql(\"select name, age from users where age > 20\").explain()\n",
    "\n",
    "\n",
    "# 5.5.2.4절\n",
    "def runColumnEx(spark, sc, df):\n",
    "    df.where(df.age > 10).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.2절\n",
    "def runAlias(spark, sc, df):\n",
    "    df.select(df.age + 1).show()\n",
    "    df.select((df.age + 1).alias(\"age\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.3절\n",
    "def runIsinEx(spark, sc):\n",
    "    nums = spark.sparkContext.broadcast([1, 3, 5, 7, 9])\n",
    "    rdd = spark.sparkContext.parallelize(range(0, 10)).map(lambda v: Row(v))\n",
    "    df = spark.createDataFrame(rdd)\n",
    "    df.where(df._1.isin(nums.value)).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.4절\n",
    "def runWhenEx(spark, sc):\n",
    "    ds = spark.range(0, 5)\n",
    "    col = when(ds.id % 2 == 0, \"even\").otherwise(\"odd\").alias(\"type\")\n",
    "    ds.select(ds.id, col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.5절\n",
    "def runMaxMin(spark, df):\n",
    "    min_col = min(\"age\")\n",
    "    max_col = max(\"age\")\n",
    "    df.select(min_col, max_col).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.6절 ~ 5.5.2.4.9절\n",
    "def runAggregateFunctions(spark, df1, df2):\n",
    "    # collect_list, collect_set\n",
    "    doubledDf1 = df1.union(df1)\n",
    "    doubledDf1.select(functions.collect_list(doubledDf1[\"name\"])).show(truncate=False)\n",
    "    doubledDf1.select(functions.collect_set(doubledDf1[\"name\"])).show(truncate=False)\n",
    "\n",
    "    # count, countDistinct\n",
    "    doubledDf1.select(functions.count(doubledDf1[\"name\"]), functions.countDistinct(doubledDf1[\"name\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "    # sum\n",
    "    df2.printSchema()\n",
    "    df2.select(sum(df2[\"price\"])).show(truncate=False)\n",
    "\n",
    "    # grouping, grouping_id\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping(df2[\"store\"])).show(truncate=False)\n",
    "    df2.cube(df2[\"store\"], df2[\"product\"]).agg(sum(df2[\"amount\"]), grouping_id(df2[\"store\"], df2[\"product\"])).show(\n",
    "        truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.10 ~ 5.5.2.4.11 절\n",
    "def runCollectionFunctions(spark):\n",
    "    df = spark.createDataFrame([{'numbers': '9,1,5,3,9'}])\n",
    "    arrayCol = split(df.numbers, \",\")\n",
    "\n",
    "    # array_contains, size\n",
    "    df.select(arrayCol, array_contains(arrayCol, 2), size(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # sort_array()\n",
    "    df.select(arrayCol, sort_array(arrayCol)).show(truncate=False)\n",
    "\n",
    "    # explode, posexplode\n",
    "    df.select(explode(arrayCol)).show(truncate=False)\n",
    "    df.select(posexplode(arrayCol)).show(truncate=False)\n",
    "\n",
    "\n",
    "# 5.5.2.4.12 ~ 5.5.2.4.14절\n",
    "def runDateFunctions(spark):\n",
    "    f1 = StructField(\"d1\", StringType(), True)\n",
    "    f2 = StructField(\"d2\", StringType(), True)\n",
    "    schema1 = StructType([f1, f2])\n",
    "\n",
    "    df = spark.createDataFrame([(\"2017-12-25 12:00:05\", \"2017-12-25\")], schema1)\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    # current_date, unix_timestamp, to_date\n",
    "    d3 = current_date().alias(\"d3\")\n",
    "    d4 = unix_timestamp(df[\"d1\"].alias(\"d4\"))\n",
    "    d5 = to_date(df[\"d2\"].alias(\"d5\"))\n",
    "    d6 = to_date(d4.cast(\"timestamp\")).alias(\"d6\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d3, d4, d5, d6).show(truncate=False)\n",
    "\n",
    "    # add_months, date_add, last_day\n",
    "    d7 = add_months(d6, 2).alias(\"d7\")\n",
    "    d8 = date_add(d6, 2).alias(\"d8\")\n",
    "    d9 = last_day(d6).alias(\"d9\")\n",
    "    df.select(df[\"d1\"], df[\"d2\"], d7, d8, d9).show(truncate=False)\n",
    "\n",
    "    # window\n",
    "    f3 = StructField(\"date\", StringType(), True)\n",
    "    f4 = StructField(\"product\", StringType(), True)\n",
    "    f5 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema2 = StructType([f3, f4, f5])\n",
    "\n",
    "    r2 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    r3 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    r4 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    r5 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    r6 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    r7 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    r8 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    r9 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([r2, r3, r4, r5, r6, r7, r8, r9], schema2);\n",
    "\n",
    "    timeCol = unix_timestamp(dd[\"date\"]).cast(\"timestamp\");\n",
    "    windowCol = window(timeCol, \"5 minutes\");\n",
    "    dd.groupBy(windowCol, dd[\"product\"]).agg(sum(dd[\"amount\"])).show(truncate=False);\n",
    "\n",
    "\n",
    "# 5.5.2.4.15절\n",
    "def runDateFunctions(spark):\n",
    "    # 파이썬의 경우 아래와 같이 튜플을 이용하여 데이터프레임을 생성하는 것도 가능함\n",
    "    df1 = spark.createDataFrame([(1.512,), (2.234,), (3.42,)], ['value'])\n",
    "    df2 = spark.createDataFrame([(25.0,), (9.0,), (10.0,)], ['value'])\n",
    "\n",
    "    df1.select(round(df1[\"value\"], 1)).show()\n",
    "    df2.select(functions.sqrt('value')).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.16 ~ 5.5.2.4.20절\n",
    "def runOtherFunctions(spark, personDf):\n",
    "    df = spark.createDataFrame([(\"v1\", \"v2\", \"v3\")], [\"c1\", \"c2\", \"c3\"]);\n",
    "\n",
    "    # array\n",
    "    df.select(df.c1, df.c2, df.c3, array(\"c1\", \"c2\", \"c3\").alias(\"newCol\")).show(truncate=False)\n",
    "\n",
    "    # desc, asc\n",
    "    personDf.show()\n",
    "    personDf.sort(functions.desc(\"age\"), functions.asc(\"name\")).show()\n",
    "\n",
    "    # pyspark 2.1.0 버전은 desc_nulls_first, desc_nulls_last, asc_nulls_first, asc_nulls_last 지원하지 않음\n",
    "\n",
    "    # split, length (pyspark에서 컬럼은 df[\"col\"] 또는 df.col 형태로 사용 가능)\n",
    "    df2 = spark.createDataFrame([(\"Splits str around pattern\",)], ['value'])\n",
    "    df2.select(df2.value, split(df2.value, \" \"), length(df2.value)).show(truncate=False)\n",
    "\n",
    "    # rownum, rank\n",
    "    f1 = StructField(\"date\", StringType(), True)\n",
    "    f2 = StructField(\"product\", StringType(), True)\n",
    "    f3 = StructField(\"amount\", IntegerType(), True)\n",
    "    schema = StructType([f1, f2, f3])\n",
    "\n",
    "    p1 = (\"2017-12-25 12:01:00\", \"note\", 1000)\n",
    "    p2 = (\"2017-12-25 12:01:10\", \"pencil\", 3500)\n",
    "    p3 = (\"2017-12-25 12:03:20\", \"pencil\", 23000)\n",
    "    p4 = (\"2017-12-25 12:05:00\", \"note\", 1500)\n",
    "    p5 = (\"2017-12-25 12:05:07\", \"note\", 2000)\n",
    "    p6 = (\"2017-12-25 12:06:25\", \"note\", 1000)\n",
    "    p7 = (\"2017-12-25 12:08:00\", \"pencil\", 500)\n",
    "    p8 = (\"2017-12-25 12:09:45\", \"note\", 30000)\n",
    "\n",
    "    dd = spark.createDataFrame([p1, p2, p3, p4, p5, p6, p7, p8], schema)\n",
    "    w1 = Window.partitionBy(\"product\").orderBy(\"amount\")\n",
    "    w2 = Window.orderBy(\"amount\")\n",
    "    dd.select(dd.product, dd.amount, functions.row_number().over(w1).alias(\"rownum\"),\n",
    "              functions.rank().over(w2).alias(\"rank\")).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.21절\n",
    "def runUDF(spark, df):\n",
    "    # functions를 이용한 등록\n",
    "    fn1 = functions.udf(lambda job: job == \"student\")\n",
    "    df.select(df[\"name\"], df[\"age\"], df[\"job\"], fn1(df[\"job\"])).show()\n",
    "    # SparkSession을 이용한 등록\n",
    "    spark.udf.register(\"fn2\", lambda job: job == \"student\")\n",
    "    df.createOrReplaceTempView(\"persons\")\n",
    "    spark.sql(\"select name, age, job, fn2(job) from persons\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.24절\n",
    "def runAgg(spark, df):\n",
    "    df.agg(max(\"amount\"), min(\"price\")).show()\n",
    "    df.agg({\"amount\": \"max\", \"price\": \"min\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.26절\n",
    "def runDfAlias(spark, df):\n",
    "    df.select(df[\"product\"]).show()\n",
    "    df.alias(\"aa\").select(\"aa.product\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.27절\n",
    "def runGroupBy(spark, df):\n",
    "    df.groupBy(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.3.4.28절\n",
    "def runCube(spark, df):\n",
    "    df.cube(\"store\", \"product\").agg({\"price\": \"sum\"}).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.29절\n",
    "def runDistinct(spark):\n",
    "    d1 = (\"store1\", \"note\", 20, 2000)\n",
    "    d2 = (\"store1\", \"bag\", 10, 5000)\n",
    "    d3 = (\"store1\", \"note\", 20, 2000)\n",
    "    rows = [d1, d2, d3]\n",
    "    cols = [\"store\", \"product\", \"amount\", \"price\"]\n",
    "    df = spark.createDataFrame(rows, cols)\n",
    "    df.distinct().show()\n",
    "    df.dropDuplicates([\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.30절\n",
    "def runDrop(spark, df):\n",
    "    df.drop(df[\"store\"]).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.31절\n",
    "def runIntersect(spark):\n",
    "    a = spark.range(1, 5)\n",
    "    b = spark.range(2, 6)\n",
    "    c = a.intersect(b)\n",
    "    c.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.32절\n",
    "def runExcept(spark):\n",
    "    df1 = spark.range(1, 6)\n",
    "    df2 = spark.createDataFrame([(2,), (4,)], ['value'])\n",
    "    # 파이썬의 경우 except 대신 subtract 메서드 사용\n",
    "    # subtract의 동작은 except와 같음\n",
    "    df1.subtract(df2).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.33절\n",
    "def runJoin(spark, ldf, rdf):\n",
    "    joinTypes = \"inner,outer,leftouter,rightouter,leftsemi\".split(\",\")\n",
    "    for joinType in joinTypes:\n",
    "        print(\"============= %s ===============\" % joinType)\n",
    "        ldf.join(rdf, [\"word\"], joinType).show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.35절\n",
    "def runNa(spark, ldf, rdf):\n",
    "    result = ldf.join(rdf, [\"word\"], \"outer\").toDF(\"word\", \"c1\", \"c2\")\n",
    "    result.show()\n",
    "    # 파이썬의 경우 na.drop또는 dropna 사용 가능\n",
    "    # c1과 c2 칼럼의 null이 아닌 값의 개수가 thresh 이하일 경우 drop\n",
    "    # thresh=1로 설정할 경우 c1 또는 c2 둘 중의 하나만 null 아닌 값을 가질 경우\n",
    "    # 결과에 포함시킨다는 의미가 됨\n",
    "    result.na.drop(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    result.dropna(thresh=2, subset=[\"c1\", \"c2\"]).show()\n",
    "    # fill\n",
    "    result.na.fill({\"c1\": 0}).show()\n",
    "    # 파이썬의 경우 to_replace에 딕셔너리를 지정하여 replace를 수행(이 경우 value에 선언한 값은 무시됨\n",
    "    # 딕셔너리를 사용하지 않을 경우 키 목록(첫번째 인자)과 값 목록(두번째 인자)을 지정하여 replace 수행\n",
    "    result.na.replace(to_replace={\"w1\": \"word1\", \"w2\": \"word2\"}, value=\"\", subset=\"word\").show()\n",
    "    result.na.replace([\"w1\", \"w2\"], [\"word1\", \"word2\"], \"word\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.36절\n",
    "def runOrderBy(spark):\n",
    "    df = spark.createDataFrame([(3, \"z\"), (10, \"a\"), (5, \"c\")], [\"idx\", \"name\"])\n",
    "    df.orderBy(\"name\", \"idx\").show()\n",
    "    df.orderBy(\"idx\", \"name\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.37절\n",
    "def runRollup(spark, df):\n",
    "    df.rollup(\"store\", \"product\").agg({\"price\": \"sum\"}).show();\n",
    "\n",
    "\n",
    "# 5.5.2.4.38절\n",
    "def runStat(spark):\n",
    "    df = spark.createDataFrame([(\"a\", 6), (\"b\", 4), (\"c\", 12), (\"d\", 6)], [\"word\", \"count\"])\n",
    "    df.show()\n",
    "    df.stat.crosstab(\"word\", \"count\").show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.39절\n",
    "def runWithColumn(spark):\n",
    "    df1 = spark.createDataFrame([(\"prod1\", \"100\"), (\"prod2\", \"200\")], [\"pname\", \"price\"])\n",
    "    df2 = df1.withColumn(\"dcprice\", df1[\"price\"] * 0.9)\n",
    "    df3 = df2.withColumnRenamed(\"dcprice\", \"newprice\")\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "    df3.show()\n",
    "\n",
    "\n",
    "# 5.5.2.4.40절\n",
    "def runSave(spark):\n",
    "    sparkHomeDir = \"file:///Users/beginspark/Apps/spark\"\n",
    "    df = spark.read.json(sparkHomeDir + \"/examples/src/main/resources/people.json\")\n",
    "    df.write.save(\"/Users/beginspark/Temp/default/%d\" % time.time())\n",
    "    df.write.format(\"json\").save(\"/Users/beginspark/Temp/json/%d\" % time.time())\n",
    "    df.write.format(\"json\").partitionBy(\"age\").save(\"/Users/beginspark/Temp/parti/%d\" % time.time())\n",
    "    # saveMode: append, overwrite, error, ignore\n",
    "    df.write.mode(\"overwrite\").saveAsTable(\"ohMyTable\")\n",
    "    spark.sql(\"select * from ohMyTable\").show()\n",
    "    # 파이썬의 경우 bucketBy 지원하지 않음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제 실행 방법] 아래에서 원하는 예제의 주석을 제거하고 실행!!\n",
    "\n",
    "# runBasicOpsEx(spark, sc, sample_df)\n",
    "# createDataFrame(spark, sc)\n",
    "#runColumnEx(spark, sc, sample_df)\n",
    "# runAlias(spark, sc, sample_df)\n",
    "# runIsinEx(spark, sc)\n",
    "# runWhenEx(spark, sc)\n",
    "# runMaxMin(spark, sample_df)\n",
    "# runAggregateFunctions(spark, sample_df, sample_df2)\n",
    "# runCollectionFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runDateFunctions(spark)\n",
    "# runOtherFunctions(spark, sample_df)\n",
    "# runUDF(spark, sample_df)\n",
    "# runAgg(spark, sample_df2)\n",
    "# runDfAlias(spark, sample_df2)\n",
    "# runGroupBy(spark, sample_df2)\n",
    "# runCube(spark, sample_df2)\n",
    "# runDistinct(spark)\n",
    "# runDrop(spark, sample_df2)\n",
    "# runIntersect(spark)\n",
    "# runExcept(spark)\n",
    "# runJoin(spark, ldf, rdf)\n",
    "# runNa(spark, ldf, rdf)\n",
    "# runOrderBy(spark)\n",
    "# runRollup(spark, sample_df2)\n",
    "# runWithColumn(spark)\n",
    "# runSave(spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
